{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nfrom init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\nfrom init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0)\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\ntrain_X, train_Y, test_X, test_Y = load_dataset()\n\ndef model(X, Y, learning_rate=0.01, num_iterations=15000, print_cost=True, initialization=\"he\"):\n    grads = {}\n    costs = []\n    m = X.shape[1]\n    layers_dims = [X.shape[0], 10, 5, 1]\n    if initialization == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n    for i in range(0, num_iterations):\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        a3, cache = forward_propagation(X, parameters)\n        cost = compute_loss(a3, Y)\n        grads = backward_propagation(X, Y, cache)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n            costs.append(cost)\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n\ndef initialize_parameters_zeros(layers_dims):\n    parameters = {}\n    L = len(layers_dims)\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n    return parameters\n\nparameters = initialize_parameters_zeros([3,2,1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\n\nparameters = model(train_X, train_Y, initialization = \"zeros\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nprint(\"predictions_train = \" + str(predictions_train))\nprint(\"predictions_test = \" + str(predictions_test))\n\nplt.title(\"Model with Zeros initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5, 1.5])\naxes.set_ylim([-1.5, 1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\ndef initialize_parameters_random(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims)\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n    return parameters\n\nparameters = initialize_parameters_random([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nparameters = model(train_X, train_Y, initialization = \"random\")\nprint(\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint(\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nprint(predictions_train)\nprint(predictions_test)\n\n\nplt.title(\"Model with large random initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5, 1.5])\naxes.set_ylim([-1.5, 1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\ndef initialize_parameters_he(layers_dims):\n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1\n    for l in range(1, L + 1):\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n    return parameters\n\nparameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\nparameters = model(train_X, train_Y, initialization = \"he\")\nprint(\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint(\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nplt.title(\"Model with He initialization\")\naxes = plt.gca()\naxes.set_xlim([-1.5, 1.5])\naxes.set_ylim([-1.5, 1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}