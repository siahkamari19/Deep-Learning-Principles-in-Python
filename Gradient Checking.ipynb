{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom testCases import *\nfrom gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n\ndef forward_propagation(x, theta):\n    J = np.dot(theta, x)\n    return J\n\nx, theta = 2, 4\nJ = forward_propagation(x, theta)\nprint (\"J = \" + str(J))\n\ndef backward_propagation(x, theta):\n    dtheta = x\n    return dtheta\n\nx, theta = 2, 4\ndtheta = backward_propagation(x, theta)\nprint (\"dtheta = \" + str(dtheta))\n\ndef gradient_check(x, theta, epsilon=1e-7):\n    thetaplus = theta + epsilon\n    thetaminus = theta - epsilon\n    J_plus = forward_propagation(x, thetaplus)\n    J_minus = forward_propagation(x, thetaminus)\n    gradapprox = (J_plus - J_minus) / (2 * epsilon)\n    grad = backward_propagation(x, theta)\n    numerator = np.linalg.norm(grad - gradapprox)\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n    difference = numerator / denominator\n    if difference < 1e-7:\n        print(\"The gradient is correct!\")\n    else:\n        print(\"The gradient is wrong!\")\n    return difference\n\nx, theta = 2, 4\ndifference = gradient_check(x, theta)\nprint(\"difference = \" + str(difference))\n\ndef forward_propagation_n(X, Y, parameters):\n    m = X.shape[1]\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n    cost = 1. / m * np.sum(logprobs)\n    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n    return cost, cache\n\ndef backward_propagation_n(X, Y, cache):\n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    dZ3 = A3 - Y\n    dW3 = 1. / m * np.dot(dZ3, A2.T)\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1. / m * np.dot(dZ1, X.T)\n    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    return gradients\n\ndef gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n    parameters_values, _ = dictionary_to_vector(parameters)\n    grad = gradients_to_vector(gradients)\n    num_parameters = parameters_values.shape[0]\n    J_plus = np.zeros((num_parameters, 1))\n    J_minus = np.zeros((num_parameters, 1))\n    gradapprox = np.zeros((num_parameters, 1))\n    for i in range(num_parameters):\n        thetaplus =  np.copy(parameters_values)\n        thetaplus[i][0] = thetaplus[i][0] + epsilon\n        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))\n        thetaminus = np.copy(parameters_values)\n        thetaminus[i][0] = thetaminus[i][0] - epsilon    \n        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))\n        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n    numerator = np.linalg.norm(grad - gradapprox)\n    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n    difference = numerator / denominator\n    if difference > 1e-7:\n        print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n    else:\n        print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n    return difference\n\nX, Y, parameters = gradient_check_n_test_case()\n\ncost, cache = forward_propagation_n(X, Y, parameters)\ngradients = backward_propagation_n(X, Y, cache)\ndifference = gradient_check_n(parameters, gradients, X, Y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}