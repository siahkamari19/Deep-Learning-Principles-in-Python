{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\nfrom reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\nfrom testCases import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0)\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\ntrain_X, train_Y, test_X, test_Y = load_2D_dataset()\n\ndef model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    grads = {}\n    costs = []\n    m = X.shape[1]\n    layers_dims = [X.shape[0], 20, 3, 1]\n    parameters = initialize_parameters(layers_dims)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob < 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n        assert(lambd == 0 or keep_prob == 1)\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob < 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n\nparameters = model(train_X, train_Y)\nprint(\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint(\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nplt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75, 0.40])\naxes.set_ylim([-0.75, 0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    cross_entropy_cost = compute_cost(A3, Y)\n    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n    cost = cross_entropy_cost + L2_regularization_cost\n    return cost\n\nA3, Y_assess, parameters = compute_cost_with_regularization_test_case()\nprint(\"cost = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    dZ3 = A3 - Y\n    dW3 = 1. / m * np.dot(dZ3, A2.T) + (lambd * W3) / m\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1. / m * np.dot(dZ2, A1.T) + (lambd * W2) / m\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1. / m * np.dot(dZ1, X.T) + (lambd * W1) / m\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    return gradients\n\n\nX_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()\n\ngrads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd=0.7)\nprint (\"dW1 = \" + str(grads[\"dW1\"]))\nprint (\"dW2 = \" + str(grads[\"dW2\"]))\nprint (\"dW3 = \" + str(grads[\"dW3\"]))\n\n\nparameters = model(train_X, train_Y, lambd=0.7)\nprint(\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint(\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\n\nplt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob=0.5):\n    np.random.seed(1)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    D1 = np.random.rand(A1.shape[0], A1.shape[1])\n    D1 = D1 < keep_prob\n    A1 = A1 * D1\n    A1 = A1 / keep_prob\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])\n    D2 = D2 < keep_probe               \n    A2 = A2 * D2\n    A2 = A2 / keep_prob\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    return A3, cache\n\nX_assess, parameters = forward_propagation_with_dropout_test_case()\n\nA3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob=0.7)\nprint (\"A3 = \" + str(A3))\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    dZ3 = A3 - Y\n    dW3 = 1. / m * np.dot(dZ3, A2.T)\n    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    dA2 = dA2 * D2\n    dA2 = dA2 / keep_prob\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1. / m * np.dot(dZ2, A1.T)\n    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n    dA1 = np.dot(W2.T, dZ2)\n    dA1 = dA1 * D1\n    dA1 = dA1 / keep_prob\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1. / m * np.dot(dZ1, X.T)\n    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    return gradients\n\nX_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()\n\ngradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob=0.8)\n\nprint (\"dA1 = \" + str(gradients[\"dA1\"]))\nprint (\"dA2 = \" + str(gradients[\"dA2\"]))\n\nparameters = model(train_X, train_Y, keep_prob=0.86, learning_rate=0.3)\nprint(\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint(\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)\n\nplt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75, 0.40])\naxes.set_ylim([-0.75, 0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}