{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom testCases_v2 import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\nnp.random.seed(1)\n\nX, Y = load_planar_dataset()\n\nplt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);\n\nshape_X = X.shape\nshape_Y = Y.shape\nm = shape_X[1]  # training set size\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))\n\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);\n\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")\n       \n def layer_sizes(X, Y):\n    n_x = X.shape[0]\n    n_h = 4\n    n_y = Y.shape[0]\n    return (n_x, n_h, n_y)      \n    \ndef initialize_parameters(n_x, n_h, n_y):\n    np.random.seed(2)\n    W1 = np.random.randn(n_h,n_x) * 0.01\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y,n_h) * 0.01\n    b2 = np.zeros((n_y,1))\n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    return parameters\n    \nn_x, n_h, n_y = initialize_parameters_test_case()\nparameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ndef forward_propagation(X, parameters):\n    W1 = parameters[\"W1\"]  # (4,2)\n    b1 = parameters[\"b1\"]  # (4,1)\n    W2 = parameters[\"W2\"]  # (1,4)\n    b2 = parameters[\"b2\"]  # (1,1)\n    Z1 = np.dot(W1,X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1) + b2\n    A2 = sigmoid(Z2)\n    assert(A2.shape == (1, X.shape[1]))\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    return A2, cache\n\nX_assess, parameters = forward_propagation_test_case()\nA2, cache = forward_propagation(X_assess, parameters)\nprint(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))\n\ndef compute_cost(A2, Y, parameters):   \n    m = Y.shape[1]\n    cost = np.float64((-1.0/m) * (np.dot(Y, np.log(A2).T) + np.dot(1-Y, np.log(1-A2).T)))\n    cost = np.squeeze(cost)\n    cost = cost.astype(float)\n    assert(isinstance(cost, float))\n    return cost\n\nA2, Y_assess, parameters = compute_cost_test_case()\nprint(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))\n\ndef backward_propagation(parameters, cache, X, Y):\n    m = X.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    dZ2 = A2 - Y\n    dW2 = (1/m) * np.dot(dZ2,A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1,2))\n    dW1 = (1/m) * np.dot(dZ1,X.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    return grads\n    \nparameters, cache, X_assess, Y_assess = backward_propagation_test_case()\ngrads = backward_propagation(parameters, cache, X_assess, Y_assess)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    return parameters\n\nparameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y, parameters)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads)\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    return parameters\n    \nX_assess, Y_assess = nn_model_test_case()\nparameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\n\ndef predict(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2 > 0.5)\n    return predictions\n    \nparameters, X_assess = predict_test_case()\npredictions = predict(parameters, X_assess)\nprint(\"predictions mean = \" + str(np.mean(predictions)))\n\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))\n\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}